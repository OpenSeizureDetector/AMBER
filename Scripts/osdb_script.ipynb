{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.interpolate import CubicSpline\n",
    "\n",
    "\n",
    "class Config:\n",
    "\n",
    "    #Globals\n",
    "    batch_size = 128\n",
    "    num_classes = 2  # classes, seizure/no seizure\n",
    "    epochs = 10     # Epoch iterations\n",
    "    length_time_step = 5\n",
    "    row_hidden = 128  # hidden neurons in conv layers\n",
    "    col_hidden = 128   # hidden neurons in the Bi-LSTM layers\n",
    "    RANDOM_SEED = 333    \n",
    "    N_TIME_STEPS = 125   # 50 records in each sequence\n",
    "    N_FEATURES = 3     # mag,hr,roi_Ratio,output\n",
    "    step = 100           # window overlap = 50 -10 = 40  (80% overlap)\n",
    "    N_CLASSES = 2      # class labels\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OsdbLabelGenerator:\n",
    "    def __init__(self, file_path, sampling_rate=25):\n",
    "        self.file_path = file_path  # Path to the JSON file\n",
    "        self.sampling_rate = sampling_rate  # Sampling rate (Hz)\n",
    "        self.df_sensordata = None  # To store the processed DataFrame\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load and flatten the JSON data into a DataFrame.\"\"\"\n",
    "        with open(self.file_path, 'r') as file:\n",
    "            raw_json = json.load(file)\n",
    "        \n",
    "        flattened_data = []\n",
    "        for attribute in raw_json:\n",
    "            user_id = attribute.get('userId', None)\n",
    "            seizure_times = attribute.get('seizureTimes', [])\n",
    "            datapoints = attribute.get('datapoints', [])\n",
    "            \n",
    "            for point in datapoints:\n",
    "                event_id = point.get('eventId', None)\n",
    "                hr = point.get('hr', [])\n",
    "                o2Sat = point.get('o2Sat', [])\n",
    "                rawData = point.get('rawData', [])\n",
    "                rawData3D = point.get('rawData3D', [])\n",
    "                alarmPhrase = point.get('alarmPhrase', None)\n",
    "                flattened_data.append({\n",
    "                    'eventId': event_id,\n",
    "                    'userId': user_id,\n",
    "                    'hr': hr,\n",
    "                    'o2Sat': o2Sat,\n",
    "                    'rawData': rawData,\n",
    "                    'rawData3D': rawData3D,\n",
    "                    'seizure_times': seizure_times,\n",
    "                    'alarmPhrase': alarmPhrase\n",
    "                })\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        self.df_sensordata = pd.DataFrame(flattened_data)\n",
    "        \n",
    "        # Add a sequential 'Id' column\n",
    "        self.df_sensordata['Id'] = range(len(self.df_sensordata))\n",
    "    \n",
    "    def calculate_fft(self, raw_data):\n",
    "        \"\"\"Calculate FFT for the raw data.\"\"\"\n",
    "        raw_data = raw_data - np.mean(raw_data)  # Remove the DC component\n",
    "        fft_result = np.fft.fft(raw_data)  # Compute FFT\n",
    "        frequencies = np.fft.fftfreq(len(raw_data), d=1/self.sampling_rate)  # Compute frequencies\n",
    "        fft_magnitude = np.abs(fft_result)  # Compute the magnitude\n",
    "        positive_frequencies = frequencies[:len(frequencies)//2]  # Only positive frequencies\n",
    "        positive_fft_magnitude = fft_magnitude[:len(frequencies)//2]  # Only positive FFT magnitudes\n",
    "        return positive_frequencies, positive_fft_magnitude\n",
    "    \n",
    "    def add_fft_column(self):\n",
    "        \"\"\"Add an FFT column to the DataFrame with zero-padding to ensure each entry has 125 values.\"\"\"\n",
    "        fft_results = []\n",
    "        for _, row in self.df_sensordata.iterrows():\n",
    "            raw_data = np.array(row['rawData'])\n",
    "            _, positive_fft_magnitude = self.calculate_fft(raw_data)  # Calculate FFT for the row\n",
    "            # Apply zero padding to ensure the FFT column has exactly 125 values\n",
    "            padded_fft = np.pad(positive_fft_magnitude, (0, 125 - len(positive_fft_magnitude)), 'constant', constant_values=0)\n",
    "            fft_results.append(list(padded_fft))  # Append padded FFT result\n",
    "        self.df_sensordata['FFT'] = fft_results\n",
    "    \n",
    "    def add_timestep_and_label(self):\n",
    "        \"\"\"Add timestep and label columns to the DataFrame.\"\"\"\n",
    "        # Add 'timestep' column in 5-second increments\n",
    "        self.df_sensordata['timestep'] = self.df_sensordata.index * 5\n",
    "\n",
    "        # Add 'label' column, initialized to 0\n",
    "        self.df_sensordata['label'] = 0\n",
    "    \n",
    "    def label_alarm_events(self):\n",
    "        \"\"\"Label the data based on alarm events.\"\"\"\n",
    "        for idx, row in self.df_sensordata.iterrows():\n",
    "            if row['alarmPhrase'] == 'ALARM':  # If alarmPhrase is ALARM\n",
    "                alarm_time = row['timestep']\n",
    "                seizure_times = row['seizure_times']\n",
    "                \n",
    "                # Process the seizure times list, assuming seizure_times are in seconds\n",
    "                for seizure in seizure_times:\n",
    "                    start_time = alarm_time + seizure  # Adjust by the seizure offset\n",
    "                    \n",
    "                    # Label the rows before and after the alarm (within the range of seizure_times)\n",
    "                    before_idx = self.df_sensordata[(self.df_sensordata['timestep'] >= start_time) & \n",
    "                                                     (self.df_sensordata['timestep'] < alarm_time)].index\n",
    "                    self.df_sensordata.loc[before_idx, 'label'] = 1  # Mark as seizure (1)\n",
    "                    \n",
    "                    # For the positive offset (after alarm)\n",
    "                    after_idx = self.df_sensordata[(self.df_sensordata['timestep'] >= alarm_time) & \n",
    "                                                    (self.df_sensordata['timestep'] <= start_time)].index\n",
    "                    self.df_sensordata.loc[after_idx, 'label'] = 1  # Mark as seizure (1)\n",
    "    \n",
    "    def process_data(self):\n",
    "        \"\"\"Process the data through all stages and return the final DataFrame.\"\"\"\n",
    "        # Step 1: Load the data\n",
    "        self.load_data()\n",
    "\n",
    "        # Step 2: Add FFT column with padding\n",
    "        self.add_fft_column()\n",
    "\n",
    "        # Step 3: Add timestep and label columns\n",
    "        self.add_timestep_and_label()\n",
    "\n",
    "        # Step 4: Label based on alarm events\n",
    "        self.label_alarm_events()\n",
    "\n",
    "        # Step 5: Drop the 'seizure_times' column\n",
    "        self.df_sensordata.drop(columns=['seizure_times'], inplace=True)\n",
    "\n",
    "        # Step 6: Ensure the DataFrame is sorted by 'Id' column\n",
    "        self.df_sensordata.sort_values(by='Id', inplace=True)\n",
    "        self.df_sensordata.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return self.df_sensordata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OsdbDataLoader:\n",
    "    def __init__(self, file_path, time_steps):\n",
    "        self.file_path = file_path\n",
    "        self.time_steps = time_steps\n",
    "        self.df_sensordata = None\n",
    "        self.load_and_process_data_from_json()\n",
    "\n",
    "    def load_and_process_data_from_json(self):\n",
    "        \"\"\"\n",
    "        Load and process OSDB data from a JSON file. This function will create a DataFrame \n",
    "        with the necessary columns and calculate FFT features.\n",
    "        \"\"\"\n",
    "        with open(self.file_path, 'r') as file:\n",
    "            raw_json = json.load(file)\n",
    "\n",
    "        # Flatten the JSON and extract the necessary data\n",
    "        flattened_data = []\n",
    "        for attribute in raw_json:\n",
    "            user_id = attribute.get('userId', None)\n",
    "            datapoints = attribute.get('datapoints', [])\n",
    "\n",
    "            for point in datapoints:\n",
    "                event_id = point.get('eventId', None)\n",
    "                hr = point.get('hr', None)\n",
    "                o2Sat = point.get('o2Sat', None)\n",
    "                rawData = point.get('rawData', [])\n",
    "                rawData3D = point.get('rawData3D', [])\n",
    "\n",
    "                # FFT calculation for rawData\n",
    "                fft_result = self.calculate_fft(rawData)\n",
    "                #Uncomment the sensor data that you want to load\n",
    "                flattened_data.append({\n",
    "                    'eventId': event_id,\n",
    "                    'userId': user_id,\n",
    "                    'hr': hr,\n",
    "                    #'o2Sat': o2Sat,\n",
    "                    'rawData': rawData,\n",
    "                    #'rawData3D': rawData3D,\n",
    "                    'FFT': fft_result  # Adding FFT column directly\n",
    "                })\n",
    "\n",
    "        # Create DataFrame from the flattened data\n",
    "        self.df_sensordata = pd.DataFrame(flattened_data)\n",
    "\n",
    "        # Apply zero padding to the FFT column to make sure all rows have 125 FFT values\n",
    "        self.df_sensordata['FFT'] = self.df_sensordata['FFT'].apply(lambda fft: np.pad(fft, (0, 125 - len(fft)), 'constant', constant_values=0) if len(fft) < 125 else fft)\n",
    "\n",
    "    def calculate_fft(self, raw_data):\n",
    "        if not raw_data:\n",
    "            return []\n",
    "\n",
    "        # Convert raw_data to numpy array\n",
    "        raw_data = np.array(raw_data)\n",
    "        # Perform FFT, remove DC component, and return magnitudes\n",
    "        raw_data = raw_data - np.mean(raw_data)  # Remove DC component\n",
    "        fft_result = np.fft.fft(raw_data)\n",
    "        fft_magnitude = np.abs(fft_result)\n",
    "        # Isolate positive frequencies\n",
    "        positive_fft_magnitude = fft_magnitude[:len(fft_magnitude) // 2]\n",
    "        \n",
    "        return positive_fft_magnitude.tolist()  # Return as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReshaper:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def reshape_data(self):\n",
    "        reshaped_rows = []\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            Id = row['Id']\n",
    "            event_id = row['eventId']\n",
    "            user_id = row['userId']\n",
    "            hr = row['hr']\n",
    "            o2Sat = row['o2Sat']\n",
    "            rawData = row['rawData']\n",
    "            rawData3D = row['rawData3D']\n",
    "            fft = row['FFT']\n",
    "            label = row['label']\n",
    "            \n",
    "            # Replicate eventId, userId, hr, o2Sat for 125 times\n",
    "            repeated_info = {\n",
    "                'Id': [Id] * 125,\n",
    "                'eventId': [event_id] * 125,\n",
    "                'userId': [user_id] * 125,\n",
    "                'hr': [hr] * 125,\n",
    "                'o2Sat': [o2Sat] * 125,\n",
    "                'label': [label] * 125\n",
    "\n",
    "            }\n",
    "            \n",
    "            # Transpose rawData and FFT\n",
    "            rawData_transposed = rawData[:125]  # Transpose to the correct shape\n",
    "            fft_transposed = fft[:125]  # Transpose to the correct shape\n",
    "            \n",
    "            # Process rawData3D if it exists\n",
    "            if rawData3D:\n",
    "                # Convert rawData3D into lists of 3 (x, y, z)\n",
    "                rawData3D_transposed = [rawData3D[i:i+3] for i in range(0, len(rawData3D), 3)]\n",
    "                rawData3D_transposed = rawData3D_transposed[:125]  # Ensure only 125 rows\n",
    "            else:\n",
    "                rawData3D_transposed = [None] * 125  # If no rawData3D, set it to None\n",
    "            \n",
    "            # Create the reshaped row\n",
    "            for i in range(125):\n",
    "                reshaped_rows.append({\n",
    "                    'Id': repeated_info['Id'][i],\n",
    "                    'eventId': repeated_info['eventId'][i],\n",
    "                    'userId': repeated_info['userId'][i],\n",
    "                    'hr': repeated_info['hr'][i],\n",
    "                    'o2Sat': repeated_info['o2Sat'][i],\n",
    "                    'rawData': rawData_transposed[i],\n",
    "                    'rawData3D': rawData3D_transposed[i],\n",
    "                    'FFT': fft_transposed[i],\n",
    "                    'label': repeated_info['label'][i],\n",
    "\n",
    "                })\n",
    "        \n",
    "        # Create a new DataFrame from the reshaped rows\n",
    "        reshaped_df = pd.DataFrame(reshaped_rows)\n",
    "        return reshaped_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interpolator:\n",
    "    def __init__(self, df, column_to_interpolate):\n",
    "        \"\"\"\n",
    "        Initialize the Interpolator class with a DataFrame and the column to interpolate.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.column_to_interpolate = column_to_interpolate\n",
    "\n",
    "    def interpolate_column(self, new_column_name='interpolated_hr', interval=125, time_step=5):\n",
    "        \"\"\"\n",
    "        Interpolate the specified column using the provided logic.\n",
    "        \n",
    "        Parameters:\n",
    "        - new_column_name: Name of the new column to store interpolated values.\n",
    "        - interval: Interval to sample the original column (e.g., every 125th element).\n",
    "        - time_step: Time step in seconds for the interpolation process.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Extract every nth element from the specified column\n",
    "            original_values = self.df[self.column_to_interpolate]\n",
    "            selected_elements = original_values[0::interval]\n",
    "            x = np.array(selected_elements)\n",
    "\n",
    "            # Step 2: Create an array representing the time (in `time_step` intervals)\n",
    "            time_values = np.arange(len(x)) * time_step\n",
    "\n",
    "            # Step 3: Create a CubicSpline object for interpolation\n",
    "            cs = CubicSpline(time_values, x, bc_type='clamped')\n",
    "\n",
    "            # Step 4: Generate new time values for finer granularity\n",
    "            num_original_points = len(x)\n",
    "            new_time_values = np.linspace(0, (num_original_points - 1) * time_step, num_original_points * interval)\n",
    "\n",
    "            # Step 5: Generate interpolated values\n",
    "            interpolated_values = cs(new_time_values)\n",
    "\n",
    "            # Step 6: Add the interpolated values to the DataFrame\n",
    "            self.df[new_column_name] = interpolated_values[:len(self.df)]  # Match the original DataFrame length\n",
    "\n",
    "            # Step 7: Rearrange columns so that 'label' is always last\n",
    "            columns = list(self.df.columns)\n",
    "            if 'label' in columns:\n",
    "                columns.remove('label')\n",
    "                columns.append('label')\n",
    "            self.df = self.df[columns]\n",
    "\n",
    "            print(f\"Interpolation completed. New column '{new_column_name}' added to the DataFrame.\")\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred during interpolation:\", e)\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        \"\"\"\n",
    "        Return the updated DataFrame with interpolated values.\n",
    "        \"\"\"\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 1. Connecting to Dataset\n",
      "INFO: Successfully connected to and processed dataset.\n",
      "INFO: 2. Reshaping Dataset\n",
      "INFO: Successfully reshaped dataset.\n",
      "INFO: 3. Interpolating Dataset\n",
      "Interpolation completed. New column 'interpolated_hr' added to the DataFrame.\n",
      "INFO: Successfully interpolated dataset.\n",
      "INFO: 4. Generated DataFrame\n",
      "   Id  eventId  userId  hr  o2Sat  rawData rawData3D           FFT  \\\n",
      "0   0      407      39  67     -1   1496.0      None  1.296030e-11   \n",
      "1   0      407      39  67     -1   1480.0      None  1.430513e+02   \n",
      "2   0      407      39  67     -1   1500.0      None  5.417937e+01   \n",
      "3   0      407      39  67     -1   1492.0      None  1.404751e+02   \n",
      "4   0      407      39  67     -1   1496.0      None  1.208051e+02   \n",
      "\n",
      "   interpolated_hr  label  \n",
      "0        67.000000      0  \n",
      "1        66.999973      0  \n",
      "2        66.999893      0  \n",
      "3        66.999761      0  \n",
      "4        66.999579      0  \n",
      "INFO: 5. Tasks Complete\n"
     ]
    }
   ],
   "source": [
    "# Configure logging for Jupyter Notebook\n",
    "def log_message(level, message):\n",
    "    print(f\"{level}: {message}\")\n",
    "\n",
    "# Main function\n",
    "def main(file_path):\n",
    "    try:\n",
    "        # Step 1: Load labeled data\n",
    "        log_message(\"INFO\", \"1. Connecting to Dataset\")\n",
    "        if not os.path.exists(file_path):\n",
    "            log_message(\"ERROR\", f\"File not found: {file_path}\")\n",
    "            return\n",
    "\n",
    "        processor = OsdbLabelGenerator(file_path)\n",
    "        df_result = processor.process_data()\n",
    "        if df_result.empty:\n",
    "            log_message(\"ERROR\", \"Loaded dataset is empty.\")\n",
    "            return\n",
    "        log_message(\"INFO\", \"Successfully connected to and processed dataset.\")\n",
    "\n",
    "        # Step 2: Reshape the flattened DataFrame\n",
    "        log_message(\"INFO\", \"2. Reshaping Dataset\")\n",
    "        reshaper = DataReshaper(df_result)\n",
    "        reshaped_df = reshaper.reshape_data()\n",
    "        if reshaped_df.empty:\n",
    "            log_message(\"ERROR\", \"Reshaping failed. DataFrame is empty after reshaping.\")\n",
    "            return\n",
    "        log_message(\"INFO\", \"Successfully reshaped dataset.\")\n",
    "\n",
    "        # Step 3: Interpolate missing data\n",
    "        log_message(\"INFO\", \"3. Interpolating Dataset\")\n",
    "        interpolator = Interpolator(reshaped_df, column_to_interpolate=\"hr\")\n",
    "        interpolator.interpolate_column(\n",
    "            new_column_name=\"interpolated_hr\",\n",
    "            interval=Config.N_TIME_STEPS,\n",
    "            time_step=Config.length_time_step,\n",
    "        )\n",
    "        dataset_df = interpolator.get_dataframe()\n",
    "        if dataset_df.empty:\n",
    "            log_message(\"ERROR\", \"Interpolation failed. DataFrame is empty after processing.\")\n",
    "            return\n",
    "        log_message(\"INFO\", \"Successfully interpolated dataset.\")\n",
    "\n",
    "        # Step 4: Display final DataFrame\n",
    "        log_message(\"INFO\", \"4. Generated DataFrame\")\n",
    "        print(dataset_df.head(5))\n",
    "        log_message(\"INFO\", \"5. Tasks Complete\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_message(\"ERROR\", f\"An error occurred: {e}\")\n",
    "\n",
    "# Example file path for the notebook\n",
    "file_path = \"../Data/osdb_3min_allSeizures.json\"\n",
    "main(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ **OSDB Data Processor Component** \n",
    "\n",
    "The **OSDB Data Processor** component is designed to process raw JSON data from the **OpenSeizure Database (OSDB)**. The component includes several modular classes located in the `OsdbDataProcessor` folder, which form a data processing pipeline. The pipeline loads raw **JSON**, reshapes the data for timeseries analysis, runs an interpolator and outputs a dataframe that can be integrated with the **AMBER Model**. \n",
    "\n",
    "### ðŸ“‚ **OsdbDataProcessor Overview**\n",
    "\n",
    "- **`Config`**: Defines the configuration parameters required throughout the data pipeline (e.g., batch size, time steps, and sampling rate).\n",
    "- **`OsdbDataLoader`**: Connects to the **OSDB** and reads raw **JSON** data to generate an unlabelled dataframe of sensor data (acceleration (vector magntiude 1D and fast fourier transform **FFT**), heart rate and sp02)\n",
    "- **`OsdbLabelGenerator`**:  Connects to the **OSDB** and reads raw **JSON** data to generate a labelled dataframe using the **seizureTimes** attribute\n",
    "- **`OsdbDataReshaper`**: Reshapes the generated dataframe into a time series format for input into the **AMBER** model.\n",
    "- **`Interpolator`**: Interpolates duplcaite **hr** values, applying cubic spline interpolation to ensure smooth and accurate data continuity between the timestep t and t+1\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **`How to Run the Data Processor (main.py)`** ðŸ”\n",
    "#### 1. **Import OSDB Data Processing Classes**:\n",
    "``` python\n",
    "from config import Config\n",
    "from OsdbDataProcessor.osdb_data_label_generator import OsdbDataLabelGenerator\n",
    "from OsdbDataProcessor.osdb_data_reshaper import OsdbDataReshaper\n",
    "from OsdbDataProcessor.osdb_interpolator import OsdbInterpolator\n",
    "from OsdbDataProcessor.osdb_data_loader import OsdbDataLoader\n",
    "```\n",
    "#### 2. **Set path to the OSDB JSON file**:\n",
    "``` python\n",
    "file_path = 'Data/osdb_3min_allSeizures.json'  # Replace with your JSON file path\n",
    "```\n",
    "\n",
    "#### 2. **Load OSDB cata label generator and pass file path**:\n",
    "``` python\n",
    "osdb_processor = OsdbDataLabelGenerator(file_path)\n",
    "df_result = processor.process_data()\n",
    "```\n",
    "\n",
    "#### 3. **Reshape the Dataframe for Timeseries Analysis**:\n",
    "``` python\n",
    "data_reshaper = OsdbDataReshaper(df_result)\n",
    "reshaped_df = data_reshaper.reshape_data()\n",
    "```\n",
    "\n",
    "#### 4. **Initialise the Interpolator and interpolate the 'hr' column**:\n",
    "``` python\n",
    "interpolator = OsdbInterpolator(reshaped_df, column_to_interpolate=\"hr\")\n",
    "interpolator.interpolate_column(new_column_name=\"interpolated_hr\", interval=config.N_TIME_STEPS, time_step=config.time_step_length)\n",
    "# Retrieve the updated DataFrame\n",
    "dataset_df = dataset_df.get_dataframe()    \n",
    "print(dataset_df.sample())#print sample row\n",
    "```\n",
    "---\n",
    "\n",
    "2. **Open Root from Integrated Terminal**:\n",
    "```bash\n",
    "path/AMBER/AMBER main python.py\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
